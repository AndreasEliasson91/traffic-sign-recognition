{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from application.src.models.faster_rcnn import load_model\n",
    "from application.src.config import TEST_DIR\n",
    "from torchvision.transforms import transforms\n",
    "from application.src.models.custom_dataset import test_dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import heapq\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = load_model()\n",
    "transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of images in path: 215\n",
      "Lengths: 215, 215, 215, 215\n"
     ]
    }
   ],
   "source": [
    "image_index = 0\n",
    "all_correct_labels = []\n",
    "all_predicted_labels = []\n",
    "all_scores = []\n",
    "all_boxes = []\n",
    "\n",
    "for subdir, dirs, files in os.walk(TEST_DIR):\n",
    "    for i, file in enumerate(files):\n",
    "        if file[-4:] == '.jpg':\n",
    "            img_path = os.path.join(subdir, file)\n",
    "            PIL_image = Image.open(img_path)\n",
    "            test_image = transform(PIL_image)\n",
    "            test_image = test_image.view(1, 3, test_image.shape[1], test_image.shape[2])\n",
    "            _, target = test_dataset[image_index]\n",
    "            correct_labels, predicted_labels, scores, boxes = model.verify(test_image, target)\n",
    "            all_correct_labels.append(correct_labels)\n",
    "            all_predicted_labels.append(predicted_labels)\n",
    "            all_scores.append(scores)\n",
    "            all_boxes.append(boxes)\n",
    "            image_index += 1\n",
    "        # if i > 10:\n",
    "        #     print('Loop ends')\n",
    "        #     break\n",
    "print(f'No of images in path: {image_index}')\n",
    "print(f'Lengths: {len(all_correct_labels)}, {len(all_predicted_labels)}, {len(all_scores)}, {len(all_boxes)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def check_guesses(list_predicted_labels, list_correct_labels):\n",
    "    # Checking for guesses and false positives + creating baseline\n",
    "    guesses = 0  # A predicted label is found among the correct labels\n",
    "    false_pos = 0  # A predicted label is not found among the correct labels\n",
    "    baseline = 0  # Baseline is the total number of correct labels to be found\n",
    "    for i, labels in enumerate(list_predicted_labels):\n",
    "        correct_labels = list_correct_labels[i]\n",
    "        baseline += len(correct_labels)\n",
    "        for label in labels:\n",
    "            if label in correct_labels:\n",
    "                guesses += 1\n",
    "                correct_labels.remove(label)\n",
    "            else:\n",
    "                false_pos += 1\n",
    "    return guesses, false_pos, baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def check_hits(list_predicted_labels, list_correct_labels, list_scores):\n",
    "    # Checking for true hits\n",
    "    hits = 0  # A top predicted label equals a correct label\n",
    "    for i, labels in enumerate(list_predicted_labels):\n",
    "        scores = list_scores[i]\n",
    "        correct_labels = list_correct_labels[i]\n",
    "        top_predictions = heapq.nlargest(len(correct_labels), zip(scores, labels))\n",
    "        for prediction in top_predictions:\n",
    "            label = prediction[1]\n",
    "            if label in correct_labels:\n",
    "                hits += 1\n",
    "                correct_labels.remove(label)\n",
    "    return hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 444, Percent of baseline: 86.0\n",
      "Uncertain predictions: 483, Percent of baseline: 93.0\n",
      "False predictions: 508, Percent of baseline: 98.0\n",
      "Baseline (number of annotated signs to find): 517\n"
     ]
    }
   ],
   "source": [
    "# These indicators do not take into account if the bounding boxes match\n",
    "list_correct_labels = copy.deepcopy(all_correct_labels)\n",
    "list_predicted_labels = copy.deepcopy(all_predicted_labels)\n",
    "list_scores = copy.deepcopy(all_scores)\n",
    "guesses, false_pos, baseline = check_guesses(list_predicted_labels, list_correct_labels)\n",
    "\n",
    "list_correct_labels = copy.deepcopy(all_correct_labels)\n",
    "hits = check_hits(list_predicted_labels, list_correct_labels, list_scores)\n",
    "\n",
    "print(f'Correct predictions: {hits}, Percent of baseline: {round((hits / baseline) * 100, 0)}')\n",
    "print(f'Uncertain predictions: {guesses}, Percent of baseline: {round((guesses / baseline) * 100, 0)}')\n",
    "print(f'False predictions: {false_pos}, Percent of baseline: {round((false_pos / baseline) * 100, 0)}')\n",
    "print(f'Baseline (number of annotated signs to find): {baseline}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "991\n",
      "517\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i, list in enumerate(all_predicted_labels):\n",
    "    for prediction in list:\n",
    "        counter += 1\n",
    "print(counter)\n",
    "\n",
    "counter = 0\n",
    "for i, list in enumerate(all_correct_labels):\n",
    "    for label in list:\n",
    "        counter += 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
