{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from application.src.models.faster_rcnn import load_model\n",
    "from application.src.config import TEST_DIR\n",
    "from torchvision.transforms import transforms\n",
    "from application.src.models.custom_dataset import test_dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import heapq\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = load_model()\n",
    "transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of images in path: 215\n",
      "Lengths: 215, 215, 215, 215\n"
     ]
    }
   ],
   "source": [
    "image_index = 0\n",
    "all_correct_labels = []\n",
    "all_predicted_labels = []\n",
    "all_scores = []\n",
    "all_boxes = []\n",
    "\n",
    "for subdir, dirs, files in os.walk(TEST_DIR):\n",
    "    for i, file in enumerate(files):\n",
    "        if file[-4:] == '.jpg':\n",
    "            img_path = os.path.join(subdir, file)\n",
    "            PIL_image = Image.open(img_path)\n",
    "            test_image = transform(PIL_image)\n",
    "            test_image = test_image.view(1, 3, test_image.shape[1], test_image.shape[2])\n",
    "            _, target = test_dataset[image_index]\n",
    "            correct_labels, predicted_labels, scores, boxes = model.verify(test_image, target)\n",
    "            all_correct_labels.append(correct_labels)\n",
    "            all_predicted_labels.append(predicted_labels)\n",
    "            all_scores.append(scores)\n",
    "            all_boxes.append(boxes)\n",
    "            image_index += 1\n",
    "        # if i > 10:\n",
    "        #     print('Loop ends')\n",
    "        #     break\n",
    "print(f'No of images in path: {image_index}')\n",
    "print(f'Lengths: {len(all_correct_labels)}, {len(all_predicted_labels)}, {len(all_scores)}, {len(all_boxes)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def check_guesses(list_predicted_labels, list_correct_labels):\n",
    "    # Checking for guesses and false positives + creating baseline\n",
    "    guesses = 0  # A predicted label is found among the correct labels\n",
    "    false_pos = 0  # A predicted label is not found among the correct labels\n",
    "    baseline = 0  # Baseline is the total number of correct labels to be found\n",
    "    for i, labels in enumerate(list_predicted_labels):\n",
    "        correct_labels = list_correct_labels[i]\n",
    "        baseline += len(correct_labels)\n",
    "        for label in labels:\n",
    "            if label in correct_labels:\n",
    "                guesses += 1\n",
    "                correct_labels.remove(label)\n",
    "            else:\n",
    "                false_pos += 1\n",
    "    return guesses, false_pos, baseline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def check_hits(list_predicted_labels, list_correct_labels, list_scores):\n",
    "    # Checking for true hits\n",
    "    hits = 0  # A top predicted label equals a correct label\n",
    "    for i, labels in enumerate(list_predicted_labels):\n",
    "        scores = list_scores[i]\n",
    "        correct_labels = list_correct_labels[i]\n",
    "        top_predictions = heapq.nlargest(len(correct_labels), zip(scores, labels))\n",
    "        for prediction in top_predictions:\n",
    "            label = prediction[1]\n",
    "            if label in correct_labels:\n",
    "                hits += 1\n",
    "                correct_labels.remove(label)\n",
    "    return hits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 444, Percent of baseline: 86.0\n",
      "Uncertain predictions: 483, Percent of baseline: 93.0\n",
      "False predictions: 508, Percent of baseline: 98.0\n",
      "Baseline (number of annotated signs to find): 517\n"
     ]
    }
   ],
   "source": [
    "# These indicators do not take into account if the bounding boxes match\n",
    "list_correct_labels = copy.deepcopy(all_correct_labels)\n",
    "list_predicted_labels = copy.deepcopy(all_predicted_labels)\n",
    "list_scores = copy.deepcopy(all_scores)\n",
    "guesses, false_pos, baseline = check_guesses(list_predicted_labels, list_correct_labels)\n",
    "\n",
    "list_correct_labels = copy.deepcopy(all_correct_labels)\n",
    "hits = check_hits(list_predicted_labels, list_correct_labels, list_scores)\n",
    "\n",
    "print(f'Correct predictions: {hits}, Percent of baseline: {round((hits / baseline) * 100, 0)}')\n",
    "print(f'Uncertain predictions: {guesses}, Percent of baseline: {round((guesses / baseline) * 100, 0)}')\n",
    "print(f'False predictions: {false_pos}, Percent of baseline: {round((false_pos / baseline) * 100, 0)}')\n",
    "print(f'Baseline (number of annotated signs to find): {baseline}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "991\n",
      "517\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i, list in enumerate(all_predicted_labels):\n",
    "    for prediction in list:\n",
    "        counter += 1\n",
    "print(counter)\n",
    "\n",
    "counter = 0\n",
    "for i, list in enumerate(all_correct_labels):\n",
    "    for label in list:\n",
    "        counter += 1\n",
    "print(counter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All correct labels (old): [['100_SIGN'], ['100_SIGN'], ['100_SIGN'], ['100_SIGN'], ['100_SIGN'], ['100_SIGN']]\n",
      "All correct labels: [['100_SIGN'], ['100_SIGN'], ['100_SIGN'], ['100_SIGN'], ['100_SIGN'], ['100_SIGN']]\n",
      "Correct labels before first loop: ['100_SIGN']\n",
      "Correct labels after first loop: []\n",
      "Accumulated guessed hits: 1, Percent of baseline: 100\n",
      "Accumulated false positives: 2, Percent of baseline: 200\n",
      "Baseline: 1\n",
      "Correct labels before first loop: ['100_SIGN']\n",
      "Correct labels after first loop: []\n",
      "Accumulated guessed hits: 2, Percent of baseline: 100\n",
      "Accumulated false positives: 5, Percent of baseline: 200\n",
      "Baseline: 2\n",
      "Correct labels before first loop: ['100_SIGN']\n",
      "Correct labels after first loop: []\n",
      "Accumulated guessed hits: 3, Percent of baseline: 100\n",
      "Accumulated false positives: 7, Percent of baseline: 200\n",
      "Baseline: 3\n",
      "Correct labels before first loop: ['100_SIGN']\n",
      "Correct labels after first loop: []\n",
      "Accumulated guessed hits: 4, Percent of baseline: 100\n",
      "Accumulated false positives: 8, Percent of baseline: 200\n",
      "Baseline: 4\n",
      "Correct labels before first loop: ['100_SIGN']\n",
      "Correct labels after first loop: []\n",
      "Accumulated guessed hits: 5, Percent of baseline: 100\n",
      "Accumulated false positives: 11, Percent of baseline: 200\n",
      "Baseline: 5\n",
      "Correct labels before first loop: ['100_SIGN']\n",
      "Correct labels after first loop: []\n",
      "Accumulated guessed hits: 6, Percent of baseline: 100\n",
      "Accumulated false positives: 13, Percent of baseline: 200\n",
      "Baseline: 6\n",
      "Correct labels before second loop: ['100_SIGN']\n",
      "Correct labels after second loop: ['100_SIGN']\n",
      "Accumulated true hits: 0\n",
      "Correct labels before second loop: ['100_SIGN']\n",
      "Correct labels after second loop: ['100_SIGN']\n",
      "Accumulated true hits: 0\n",
      "Correct labels before second loop: ['100_SIGN']\n",
      "Correct labels after second loop: []\n",
      "Accumulated true hits: 1\n",
      "Correct labels before second loop: ['100_SIGN']\n",
      "Correct labels after second loop: []\n",
      "Accumulated true hits: 2\n",
      "Correct labels before second loop: ['100_SIGN']\n",
      "Correct labels after second loop: []\n",
      "Accumulated true hits: 3\n",
      "Correct labels before second loop: ['100_SIGN']\n",
      "Correct labels after second loop: []\n",
      "Accumulated true hits: 4\n",
      "All correct labels (old): [['100_SIGN'], ['100_SIGN'], ['100_SIGN'], ['100_SIGN'], ['100_SIGN'], ['100_SIGN']]\n",
      "All correct labels: [['100_SIGN'], ['100_SIGN'], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "# These indicators do not take into account if the bounding boxes match\n",
    "list_correct_labels = copy.deepcopy(all_correct_labels)\n",
    "list_predicted_labels = copy.deepcopy(all_predicted_labels)\n",
    "list_scores = copy.deepcopy(all_scores)\n",
    "\n",
    "\n",
    "list_correct_labels = copy.deepcopy(all_correct_labels)\n",
    "print(f'Accumulated true hits: {hits}, Percent of baseline: {int(hits / baseline) * 100}')\n",
    "print(f'Accumulated guessed hits: {guesses}, Percent of baseline: {int(guesses / baseline) * 100}')\n",
    "print(f'Accumulated false positives: {false_pos}, Percent of baseline: {int(false_pos / baseline) * 100}')\n",
    "print(f'Baseline: {baseline}')\n",
    "print(f'All correct labels (old): {all_correct_labels}')\n",
    "print(f'All correct labels: {list_correct_labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}