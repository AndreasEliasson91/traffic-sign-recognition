{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from application.src.models.faster_rcnn import load_model, RCNNModel\n",
    "from application.src.config import TEST_DIR\n",
    "from torchvision.transforms import transforms\n",
    "from application.src.models.custom_dataset import test_dataset\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "model = load_model()\n",
    "transform = transforms.Compose([transforms.ToTensor()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0, Image path: C:\\Users\\Kajsa\\PycharmProjects\\traffic-sign-recognition\\application/data/datasets/LU-data/test\\1277104195Image000003_jpg.rf.985b035268987fa9c74999f0db77c6b5.jpg\n",
      "Correct labels: ['100_SIGN']\n",
      "Predicted labels: ['80_SIGN', '100_SIGN', '70_SIGN']\n",
      "Scores: [84, 17, 8]\n",
      "Boxes: [tensor([167.2503, 211.0037, 177.4527, 220.4435]), tensor([167.6144, 210.5826, 177.6667, 220.4621]), tensor([167.3387, 210.7373, 177.6348, 221.5271])]\n",
      "i = 2, Image path: C:\\Users\\Kajsa\\PycharmProjects\\traffic-sign-recognition\\application/data/datasets/LU-data/test\\1277104195Image000003_jpg.rf.d6a7728a1c45d3746b093c0ca5917553.jpg\n",
      "Correct labels: ['100_SIGN']\n",
      "Predicted labels: ['80_SIGN', '70_SIGN', 'PRIORITY_ROAD', '100_SIGN']\n",
      "Scores: [82, 25, 14, 11]\n",
      "Boxes: [tensor([357.7505, 448.9753, 379.6328, 470.6000]), tensor([357.3932, 449.2030, 379.7569, 472.8456]), tensor([367.2266, 472.1650, 379.8308, 483.8214]), tensor([358.1158, 448.8360, 379.1151, 471.0172])]\n",
      "i = 4, Image path: C:\\Users\\Kajsa\\PycharmProjects\\traffic-sign-recognition\\application/data/datasets/LU-data/test\\1277104195Image000013_jpg.rf.2673d683e84593f365c512a20fad29d9.jpg\n",
      "Correct labels: ['100_SIGN']\n",
      "Predicted labels: ['100_SIGN', '80_SIGN', '70_SIGN']\n",
      "Scores: [65, 51, 13]\n",
      "Boxes: [tensor([394.7824, 440.6157, 422.5668, 468.2801]), tensor([394.2607, 440.7568, 422.2703, 468.1173]), tensor([394.6417, 440.4725, 423.0581, 470.7775])]\n",
      "Loop ends\n"
     ]
    }
   ],
   "source": [
    "for subdir, dirs, files in os.walk(TEST_DIR):\n",
    "    for i, file in enumerate(files):\n",
    "        if file[-4:] == '.jpg':\n",
    "            img_path = os.path.join(subdir, file)\n",
    "            print(f'i = {i}, Image path: {img_path}')\n",
    "            PIL_image = Image.open(img_path)\n",
    "            test_image = transform(PIL_image)\n",
    "            test_image = test_image.view(1, 3, test_image.shape[1], test_image.shape[2])\n",
    "            _, target = test_dataset[i]\n",
    "            correct_labels, predicted_labels, scores, boxes = model.verify(test_image, target)\n",
    "            print(f'Correct labels: {correct_labels}')\n",
    "            print(f'Predicted labels: {predicted_labels}')\n",
    "            print(f'Scores: {scores}')\n",
    "            print(f'Boxes: {boxes}')\n",
    "            if i > 3:\n",
    "                print('Loop ends')\n",
    "                break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "images is expected to be a list of 3d tensors of shape [C, H, W], got torch.Size([600, 3])",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [11]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mi: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      3\u001B[0m image, target \u001B[38;5;241m=\u001B[39m test_dataset[i]\n\u001B[1;32m----> 4\u001B[0m correct_labels, predicted_labels, scores, boxes \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverify\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCorrect labels: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcorrect_labels\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPredicted labels: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpredicted_labels\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\traffic-sign-recognition\\application\\src\\models\\faster_rcnn.py:28\u001B[0m, in \u001B[0;36mRCNNModel.verify\u001B[1;34m(self, image, target)\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;124;03mVerify the input image against its target labels and bounding boxes\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;124;03m:param image: PIL image; input image\u001B[39;00m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;124;03m:param target: dict; dictionary with target labels etc.\u001B[39;00m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;124;03m:return: tuple[list, list, list[int], list]; correct labels, predicted labels, scores and bounding boxes\u001B[39;00m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 28\u001B[0m     predictions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     30\u001B[0m     correct_labels \u001B[38;5;241m=\u001B[39m [CLASSES[label] \u001B[38;5;28;01mfor\u001B[39;00m label \u001B[38;5;129;01min\u001B[39;00m target[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m]]\n\u001B[0;32m     31\u001B[0m     predicted_labels \u001B[38;5;241m=\u001B[39m [CLASSES[label] \u001B[38;5;28;01mfor\u001B[39;00m label \u001B[38;5;129;01min\u001B[39;00m predictions[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m]]\n",
      "File \u001B[1;32m~\\PycharmProjects\\traffic-sign-recognition\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\PycharmProjects\\traffic-sign-recognition\\venv\\lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:78\u001B[0m, in \u001B[0;36mGeneralizedRCNN.forward\u001B[1;34m(self, images, targets)\u001B[0m\n\u001B[0;32m     75\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(val) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[0;32m     76\u001B[0m     original_image_sizes\u001B[38;5;241m.\u001B[39mappend((val[\u001B[38;5;241m0\u001B[39m], val[\u001B[38;5;241m1\u001B[39m]))\n\u001B[1;32m---> 78\u001B[0m images, targets \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     80\u001B[0m \u001B[38;5;66;03m# Check for degenerate boxes\u001B[39;00m\n\u001B[0;32m     81\u001B[0m \u001B[38;5;66;03m# TODO: Move this to a function\u001B[39;00m\n\u001B[0;32m     82\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m targets \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\traffic-sign-recognition\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\PycharmProjects\\traffic-sign-recognition\\venv\\lib\\site-packages\\torchvision\\models\\detection\\transform.py:126\u001B[0m, in \u001B[0;36mGeneralizedRCNNTransform.forward\u001B[1;34m(self, images, targets)\u001B[0m\n\u001B[0;32m    123\u001B[0m target_index \u001B[38;5;241m=\u001B[39m targets[i] \u001B[38;5;28;01mif\u001B[39;00m targets \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    125\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m image\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m3\u001B[39m:\n\u001B[1;32m--> 126\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimages is expected to be a list of 3d tensors of shape [C, H, W], got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimage\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    127\u001B[0m image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnormalize(image)\n\u001B[0;32m    128\u001B[0m image, target_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresize(image, target_index)\n",
      "\u001B[1;31mValueError\u001B[0m: images is expected to be a list of 3d tensors of shape [C, H, W], got torch.Size([600, 3])"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_dataset)):\n",
    "    print(f'i: {i}')\n",
    "    image, target = test_dataset[i]\n",
    "    correct_labels, predicted_labels, scores, boxes = model.verify(image, target)\n",
    "    print(f'Correct labels: {correct_labels}')\n",
    "    print(f'Predicted labels: {predicted_labels}')\n",
    "    print(f'Scores: {scores}')\n",
    "    print(f'Boxes: {boxes}')\n",
    "    if i > 3:\n",
    "        print('Loop ends')\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}