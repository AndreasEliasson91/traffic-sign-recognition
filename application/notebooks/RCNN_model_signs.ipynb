{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "RCNN-model signs",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0f5130a84c0d46f09cbc1bb632d64c82": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d30266ad4c1d4a3eadbece8240e3ac1a",
       "IPY_MODEL_f40b84a978af4b28a69c18461afd94a7",
       "IPY_MODEL_ccc8699f474b43228f6fbc507d25e2d2"
      ],
      "layout": "IPY_MODEL_550e2617e69d4736b18e841af0bc0aea"
     }
    },
    "d30266ad4c1d4a3eadbece8240e3ac1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1b6047bc31c48aa8fb1d8b32bc052d3",
      "placeholder": "​",
      "style": "IPY_MODEL_827a01780fb8434891f73b00b782a62f",
      "value": "Loss: 0.1216: 100%"
     }
    },
    "f40b84a978af4b28a69c18461afd94a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94cb9e48c7a940579fc6dae392ff25b6",
      "max": 1195,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_253ec517bfa04f2c9c3c71e7b2e0125c",
      "value": 1195
     }
    },
    "ccc8699f474b43228f6fbc507d25e2d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_35c432aa497b45deb25b45b74d7bacdc",
      "placeholder": "​",
      "style": "IPY_MODEL_6b75cde78f414f45844939ba81963728",
      "value": " 1195/1195 [09:02&lt;00:00,  2.28it/s]"
     }
    },
    "550e2617e69d4736b18e841af0bc0aea": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1b6047bc31c48aa8fb1d8b32bc052d3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "827a01780fb8434891f73b00b782a62f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "94cb9e48c7a940579fc6dae392ff25b6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "253ec517bfa04f2c9c3c71e7b2e0125c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "35c432aa497b45deb25b45b74d7bacdc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b75cde78f414f45844939ba81963728": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c1e6821accd3498f8e3a2d8f47f0ed2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_24d0a32d6be440b7b5e6e15ef78e5fae",
       "IPY_MODEL_96661718d74f435fb71d058fba2cd0ca",
       "IPY_MODEL_e3781629dcb24f11a316e355470eae41"
      ],
      "layout": "IPY_MODEL_3976d20949dd4d4f995b94ebfcf3dc64"
     }
    },
    "24d0a32d6be440b7b5e6e15ef78e5fae": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45d252f6c23c47b2b5ef157191d79daa",
      "placeholder": "​",
      "style": "IPY_MODEL_00e7b9a3f50547d387e5dc9f22adecd9",
      "value": "Loss: 0.6555: 100%"
     }
    },
    "96661718d74f435fb71d058fba2cd0ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_965bb49708f14846a8da417e822aab4b",
      "max": 115,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a71436b06aa04d69b000fae56b81b989",
      "value": 115
     }
    },
    "e3781629dcb24f11a316e355470eae41": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_70cf858f1c3d438099f8567cacf3f680",
      "placeholder": "​",
      "style": "IPY_MODEL_18fdfee692124066bdd0aac43e4e70e0",
      "value": " 115/115 [03:15&lt;00:00,  1.40s/it]"
     }
    },
    "3976d20949dd4d4f995b94ebfcf3dc64": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45d252f6c23c47b2b5ef157191d79daa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "00e7b9a3f50547d387e5dc9f22adecd9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "965bb49708f14846a8da417e822aab4b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a71436b06aa04d69b000fae56b81b989": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "70cf858f1c3d438099f8567cacf3f680": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "18fdfee692124066bdd0aac43e4e70e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "791bafc1540345da8eb85e7959f6714c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3597a466af2640e3a1867ea64517a729",
       "IPY_MODEL_eae5c379039e4900a7c0b4a858855946",
       "IPY_MODEL_30f836a3300b473993e6f96b91d44e28"
      ],
      "layout": "IPY_MODEL_f0c9fb6eb7cf4c2487ca93c83dc81175"
     }
    },
    "3597a466af2640e3a1867ea64517a729": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c6eea74c28b4cd79f0fb091ee53fc3a",
      "placeholder": "​",
      "style": "IPY_MODEL_7640c6b1ab3745cfa665944ae63f15c7",
      "value": "Loss: 0.3662:  50%"
     }
    },
    "eae5c379039e4900a7c0b4a858855946": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_683d1211a6f74d0494b5e2c7458a9d10",
      "max": 1195,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b691140dfdd741079aee5d97c8dcfe7f",
      "value": 595
     }
    },
    "30f836a3300b473993e6f96b91d44e28": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8077eee7552f4e63aa1294979c7769d4",
      "placeholder": "​",
      "style": "IPY_MODEL_575cfad9fa9246068892d89d2a8a5f98",
      "value": " 595/1195 [04:03&lt;04:04,  2.45it/s]"
     }
    },
    "f0c9fb6eb7cf4c2487ca93c83dc81175": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c6eea74c28b4cd79f0fb091ee53fc3a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7640c6b1ab3745cfa665944ae63f15c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "683d1211a6f74d0494b5e2c7458a9d10": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b691140dfdd741079aee5d97c8dcfe7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8077eee7552f4e63aa1294979c7769d4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "575cfad9fa9246068892d89d2a8a5f98": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Configs"
   ],
   "metadata": {
    "id": "2McS2O6zaKCR",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Pip installation\n",
    "\n",
    "# !pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "# # !pip3 install torch==1.2.0+cu92 torchvision==0.4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !pip install albumentations==0.4.6\n",
    "# !pip install opencv-python-headless==4.5.2.52"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t2KpTyPcOSlG",
    "outputId": "6680a422-9e5d-4b56-c2c7-7a8660284cb6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#IMPORTS\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob as glob\n",
    "\n",
    "#Torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from xml.etree import ElementTree as et\n",
    "from tqdm.auto import tqdm\n",
    "# from google.colab.patches import cv2_imshow\n",
    "\n",
    "plt.style.use('ggplot')"
   ],
   "metadata": {
    "id": "RJgHNVHki9qU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eVXTh07xZ_Uc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Configs\n",
    "\n",
    "BATCH_SIZE = 2 \n",
    "NUM_EPOCHS = 2\n",
    "RESIZE_TO = 400\n",
    "\n",
    "\n",
    "\n",
    "# Data folders\n",
    "TRAIN_DIR = '../data/datasets/LU-data/train'\n",
    "VALID_DIR = '../data/datasets/LU-data/valid'\n",
    "\n",
    "# location to save model and plots\n",
    "OUT_DIR = '/RCNN_results'\n",
    "\n",
    "MODEL_NAME = 'model v0.1'\n",
    "SAVE_PLOTS_EPOCH = 2 # save loss plots after these many epochs\n",
    "SAVE_MODEL_EPOCH = 2 # save model after these many epochs\n",
    "\n",
    "\n",
    "# Dataset information\n",
    "CLASSES = [\n",
    "    'BACKGROUND', 'PEDESTRIAN_CROSSING', 'PRIORITY_ROAD', 'PASS_RIGHT_SIDE', '50_SIGN', '70_SIGN', '80_SIGN', 'NO_PARKING',\n",
    "     'GIVE_WAY', '100_SIGN', 'NO_STOPPING_NO_STANDING', 'STOP', 'WARNING_BAD_ROAD', 'WARNING_DANGER','WARNING_RAIL_CROSSING',\n",
    "     'WARNING_CROSSING_ROAD', 'WARNING_PEDESTRIANS', 'WARNING', '30_SIGN','WARNING_CHILDREN', 'WARNING_SHARP_CURVES',\n",
    "     'WARNING_EQUISTARIANS', 'PASS_EITHER_SIDE']\n",
    "NUM_CLASSES = 23\n",
    "\n",
    "\n",
    "# Vizualization of results...\n",
    "VISUALIZE_TRANSFORMED_IMAGES = False\n",
    "\n",
    "#CUDA Settings\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class Averager:\n",
    " # Set everything to 0 when loaded\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "        \n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0"
   ],
   "metadata": {
    "id": "cOTZYhiEfnWd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ],
   "metadata": {
    "id": "SiB0jEJYju71",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformation on data...\n",
    "\n",
    "This will convert the images in our dataset to Tensors.\n",
    "\n",
    "We can also add any augmentations (Transformations) to the data in here.\n",
    "\n",
    "\n",
    "example:\n",
    "[\n",
    " * A.Flip(float),\n",
    " * A.RandomRotate90(float),\n",
    " * A.MotionBlur(p=float),\n",
    " * A.MedianBlur(blur_limit=int, p=float),\n",
    " * A.Blur(blur_limit=int, p=float),\n",
    " * ToTensorV2(p=1.0)\n",
    "]"
   ],
   "metadata": {
    "id": "tQVUZG5TkHmv",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def show_transformed_image(train_loader):\n",
    "\n",
    "    if len(train_loader) > 0:\n",
    "        for i in range(1):\n",
    "            images, targets = next(iter(train_loader))\n",
    "            \n",
    "            images = list(image.to(DEVICE) for image in images)\n",
    "            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n",
    "            sample = images[i].permute(1, 2, 0).cpu().numpy()\n",
    "            \n",
    "            for box in boxes:\n",
    "                cv2.rectangle(sample,\n",
    "                            (box[0], box[1]),\n",
    "                            (box[2], box[3]),\n",
    "                            (0, 0, 255), 2)\n",
    "            cv2.imshow('sample', sample)"
   ],
   "metadata": {
    "id": "KO5umbyslfqN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_train_transform():\n",
    "    return A.Compose([ToTensorV2(p=1.0)], \n",
    "                     bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))"
   ],
   "metadata": {
    "id": "P8VuG0i8j1Z3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_valid_transform():\n",
    "    return A.Compose([ToTensorV2(p=1.0)], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))"
   ],
   "metadata": {
    "id": "zhdEwvfFLr5-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class LUDataset(Dataset):\n",
    "    def __init__(self, dir_path, width, height, classes, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.dir_path = dir_path\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.classes = classes\n",
    "        \n",
    "        # get all the image paths in sorted order\n",
    "        self.image_paths = glob.glob(f'{self.dir_path}/*.jpg')\n",
    "        # self.all_images = [image_path.split('/')[-1] for image_path in self.image_paths]\n",
    "        self.all_images = [image_path.split('\\\\')[-1] for image_path in self.image_paths]  # Changed since split was in wrong place => __getitem__ dit not work.\n",
    "        self.all_images = sorted(self.all_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # capture the image name and the full image path\n",
    "        image_name = self.all_images[idx]\n",
    "        image_path = os.path.join(self.dir_path, image_name)\n",
    "\n",
    "        # read the image\n",
    "        image = cv2.imread(image_path)\n",
    "        # convert BGR to RGB color format\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image_resized = cv2.resize(image, (self.width, self.height))\n",
    "        image_resized /= 255.0\n",
    "        \n",
    "        # capture the corresponding XML file for getting the annotations\n",
    "        annot_filename = image_name[:-4] + '.xml'\n",
    "        annot_file_path = os.path.join(self.dir_path, annot_filename)\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        tree = et.parse(annot_file_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # get the height and width of the image\n",
    "        image_width = image.shape[1]\n",
    "        image_height = image.shape[0]\n",
    "        \n",
    "        # box coordinates for xml files are extracted and corrected for image size given\n",
    "        for member in root.findall('object'):\n",
    "            # map the current object name to `classes` list to get...\n",
    "            # ... the label index and append to `labels` list\n",
    "            labels.append(self.classes.index(member.find('name').text))\n",
    "            \n",
    "            # xmin = left corner x-coordinates\n",
    "            xmin = int(member.find('bndbox').find('xmin').text)\n",
    "            # xmax = right corner x-coordinates\n",
    "            xmax = int(member.find('bndbox').find('xmax').text)\n",
    "            # ymin = left corner y-coordinates\n",
    "            ymin = int(member.find('bndbox').find('ymin').text)\n",
    "            # ymax = right corner y-coordinates\n",
    "            ymax = int(member.find('bndbox').find('ymax').text)\n",
    "            \n",
    "            # resize the bounding boxes according to the...\n",
    "            # ... desired `width`, `height`\n",
    "            xmin_final = (xmin/image_width)*self.width\n",
    "            xmax_final = (xmax/image_width)*self.width\n",
    "            ymin_final = (ymin/image_height)*self.height\n",
    "            yamx_final = (ymax/image_height)*self.height\n",
    "            \n",
    "            boxes.append([xmin_final, ymin_final, xmax_final, yamx_final])\n",
    "        \n",
    "        # bounding box to tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # area of the bounding boxes\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # no crowd instances\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "        # labels to tensor\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        # prepare the final `target` dictionary\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        image_id = torch.tensor([idx])\n",
    "        target[\"image_id\"] = image_id\n",
    "\n",
    "        # apply the image transforms\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(image = image_resized,\n",
    "                                     bboxes = target['boxes'],\n",
    "                                     labels = labels)\n",
    "            image_resized = sample['image']\n",
    "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
    "            \n",
    "        return image_resized, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_images)\n",
    "\n",
    "# prepare the final datasets and data loaders\n",
    "train_dataset = LUDataset(TRAIN_DIR, RESIZE_TO, RESIZE_TO, CLASSES, get_train_transform())\n",
    "valid_dataset = LUDataset(VALID_DIR, RESIZE_TO, RESIZE_TO, CLASSES, get_valid_transform())\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(valid_dataset)}\\n\")\n",
    "\n",
    "# execute datasets.py using Python command from Terminal...\n",
    "# ... to visualize sample images\n",
    "# USAGE: python datasets.py\n",
    "if __name__ == '__main__':\n",
    "    # sanity check of the Dataset pipeline with sample visualization\n",
    "    dataset = LUDataset(\n",
    "        TRAIN_DIR, RESIZE_TO, RESIZE_TO, CLASSES\n",
    "    )\n",
    "    print(f\"Number of training images: {len(dataset)}\")\n",
    "    \n",
    "    # function to visualize a single sample\n",
    "    def visualize_sample(image, target):\n",
    "        box = target['boxes'][0]\n",
    "        label = CLASSES[target['labels']]\n",
    "        cv2.rectangle(\n",
    "            image, \n",
    "            (int(box[0]), int(box[1])), (int(box[2]), int(box[3])),\n",
    "            (0, 255, 0), 2\n",
    "        )\n",
    "        cv2.putText(\n",
    "            image, label, (int(box[0]), int(box[1]-5)), \n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2\n",
    "        )\n",
    "        cv2.imshow('image', image)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyWindow('image')\n",
    "        \n",
    "    NUM_SAMPLES_TO_VISUALIZE = 2\n",
    "    for i in range(NUM_SAMPLES_TO_VISUALIZE):\n",
    "        image, target = dataset[i]\n",
    "        visualize_sample(image, target)"
   ],
   "metadata": {
    "id": "Bd5sO1pFocdL",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "c2d40f1e-cd46-4587-d0dc-d369a89bd0d2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 801\n",
      "Number of validation samples: 229\n",
      "\n",
      "Number of training images: 801\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Dataloader\n",
    "\n",
    "train_dataset = LUDataset(TRAIN_DIR, RESIZE_TO, RESIZE_TO, CLASSES, get_train_transform())\n",
    "valid_dataset = LUDataset(VALID_DIR, RESIZE_TO, RESIZE_TO, CLASSES, get_valid_transform())\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(valid_dataset)}\\n\")"
   ],
   "metadata": {
    "id": "laDl3Q2PKEs7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "61f9435f-c18f-4a40-b83a-25710d9858f3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 801\n",
      "Number of validation samples: 229\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = LUDataset(TRAIN_DIR,\n",
    "                    RESIZE_TO,\n",
    "                    RESIZE_TO,\n",
    "                    CLASSES)\n",
    "\n",
    "print(f\"Number of training images: {len(dataset)}\")\n",
    "\n",
    "# function to visualize a single sample\n",
    "def visualize_sample(image, target):\n",
    "    box = target['boxes'][0]\n",
    "    label = CLASSES[target['labels']]\n",
    "    cv2.rectangle(\n",
    "        image,\n",
    "        (int(box[0]), int(box[1])), (int(box[2]), int(box[3])),\n",
    "        (0, 255, 0), 1\n",
    "    )\n",
    "    cv2.putText(\n",
    "        image, label, (int(box[0]), int(box[1]-5)),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2\n",
    "    )\n",
    "    cv2.imshow(\"image\", image)\n",
    "\n",
    "\n",
    "NUM_SAMPLES_TO_VISUALIZE = 2\n",
    "for i in range(NUM_SAMPLES_TO_VISUALIZE):\n",
    "    image, target = dataset[i]\n",
    "    visualize_sample(image, target)"
   ],
   "metadata": {
    "id": "a8C--KkWMcnX",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "86ca310f-232b-471f-c99b-3bcdabf99641",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 801\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Faster - RCNN model"
   ],
   "metadata": {
    "id": "ppIXPUWpMwev",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def create_model(num_classes):\n",
    "    \n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model"
   ],
   "metadata": {
    "id": "09uVIw6QMrbb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def train(train_data_loader, model):\n",
    "    print('Training')\n",
    "    global train_itr\n",
    "    global train_loss_list\n",
    "    \n",
    "    prog_bar = tqdm(train_data_loader, total=len(train_data_loader))\n",
    "    \n",
    "    for i, data in enumerate(prog_bar):\n",
    "        optimizer.zero_grad()\n",
    "        images, targets = data\n",
    "        \n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        train_loss_list.append(loss_value)\n",
    "        train_loss_hist.send(loss_value)\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        train_itr += 1\n",
    "    \n",
    "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
    "    return train_loss_list"
   ],
   "metadata": {
    "id": "u6JQCDzwNUau",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def validate(valid_data_loader, model):\n",
    "    print('Validating')\n",
    "    global val_itr\n",
    "    global val_loss_list\n",
    "    \n",
    "    prog_bar = tqdm(valid_data_loader, total=len(valid_data_loader))\n",
    "    \n",
    "    for i, data in enumerate(prog_bar):\n",
    "        images, targets = data\n",
    "        \n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss_dict = model(images, targets)\n",
    "            \n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        val_loss_list.append(loss_value)\n",
    "        val_loss_hist.send(loss_value)\n",
    "        val_itr += 1\n",
    "\n",
    "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
    "    return val_loss_list"
   ],
   "metadata": {
    "id": "R-NBQKnSlBLd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1 of 2\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/401 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ed49d6b6b384b03854fa8c3889d7d32"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Expected x_max for bbox (tensor(0.8984), tensor(0.2458), tensor(1.0008), tensor(0.4271), tensor(8)) to be in the range [0.0, 1.0], got 1.0007812976837158.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [15]\u001B[0m, in \u001B[0;36m<cell line: 20>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     27\u001B[0m figure_2, valid_ax \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39msubplots()\n\u001B[0;32m     29\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m---> 30\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m val_loss \u001B[38;5;241m=\u001B[39m validate(valid_loader, model)\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch #\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m train loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss_hist\u001B[38;5;241m.\u001B[39mvalue\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Input \u001B[1;32mIn [13]\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(train_data_loader, model)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mglobal\u001B[39;00m train_loss_list\n\u001B[0;32m      6\u001B[0m prog_bar \u001B[38;5;241m=\u001B[39m tqdm(train_data_loader, total\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(train_data_loader))\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(prog_bar):\n\u001B[0;32m      9\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     10\u001B[0m     images, targets \u001B[38;5;241m=\u001B[39m data\n",
      "File \u001B[1;32m~\\PycharmProjects\\traffic-sign-recognition\\venv\\lib\\site-packages\\tqdm\\notebook.py:258\u001B[0m, in \u001B[0;36mtqdm_notebook.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    256\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    257\u001B[0m     it \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m(tqdm_notebook, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__iter__\u001B[39m()\n\u001B[1;32m--> 258\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m it:\n\u001B[0;32m    259\u001B[0m         \u001B[38;5;66;03m# return super(tqdm...) will not catch exception\u001B[39;00m\n\u001B[0;32m    260\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[0;32m    261\u001B[0m \u001B[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\traffic-sign-recognition\\venv\\lib\\site-packages\\tqdm\\std.py:1195\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1192\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[0;32m   1194\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1195\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[0;32m   1196\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[0;32m   1197\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[0;32m   1198\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\traffic-sign-recognition\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    528\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    529\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()\n\u001B[1;32m--> 530\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    531\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    532\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    533\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    534\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\PycharmProjects\\traffic-sign-recognition\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    568\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    569\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 570\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    571\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    572\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data)\n",
      "File \u001B[1;32m~\\PycharmProjects\\traffic-sign-recognition\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[1;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\PycharmProjects\\traffic-sign-recognition\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[1;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Input \u001B[1;32mIn [8]\u001B[0m, in \u001B[0;36mLUDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     82\u001B[0m \u001B[38;5;66;03m# apply the image transforms\u001B[39;00m\n\u001B[0;32m     83\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m---> 84\u001B[0m     sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransforms\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mimage_resized\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     85\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mbboxes\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mboxes\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     86\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     87\u001B[0m     image_resized \u001B[38;5;241m=\u001B[39m sample[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     88\u001B[0m     target[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mboxes\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor(sample[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbboxes\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[1;32m~\\PycharmProjects\\traffic-sign-recognition\\venv\\lib\\site-packages\\albumentations\\core\\composition.py:207\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, force_apply, *args, **data)\u001B[0m\n\u001B[0;32m    202\u001B[0m check_each_transform \u001B[38;5;241m=\u001B[39m \u001B[38;5;28many\u001B[39m(\n\u001B[0;32m    203\u001B[0m     \u001B[38;5;28mgetattr\u001B[39m(item\u001B[38;5;241m.\u001B[39mparams, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheck_each_transform\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocessors\u001B[38;5;241m.\u001B[39mvalues()\n\u001B[0;32m    204\u001B[0m )\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocessors\u001B[38;5;241m.\u001B[39mvalues():\n\u001B[1;32m--> 207\u001B[0m     \u001B[43mp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpreprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    209\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx, t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(transforms):\n\u001B[0;32m    210\u001B[0m     data \u001B[38;5;241m=\u001B[39m t(force_apply\u001B[38;5;241m=\u001B[39mforce_apply, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdata)\n",
      "File \u001B[1;32m~\\PycharmProjects\\traffic-sign-recognition\\venv\\lib\\site-packages\\albumentations\\core\\utils.py:84\u001B[0m, in \u001B[0;36mDataProcessor.preprocess\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m     82\u001B[0m rows, cols \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mshape[:\u001B[38;5;241m2\u001B[39m]\n\u001B[0;32m     83\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data_name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_fields:\n\u001B[1;32m---> 84\u001B[0m     data[data_name] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck_and_convert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[43mdata_name\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcols\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdirection\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mto\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\traffic-sign-recognition\\venv\\lib\\site-packages\\albumentations\\core\\utils.py:92\u001B[0m, in \u001B[0;36mDataProcessor.check_and_convert\u001B[1;34m(self, data, rows, cols, direction)\u001B[0m\n\u001B[0;32m     89\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m data\n\u001B[0;32m     91\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m direction \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m---> 92\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_to_albumentations\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     94\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconvert_from_albumentations(data, rows, cols)\n",
      "File \u001B[1;32m~\\PycharmProjects\\traffic-sign-recognition\\venv\\lib\\site-packages\\albumentations\\augmentations\\bbox_utils.py:51\u001B[0m, in \u001B[0;36mBboxProcessor.convert_to_albumentations\u001B[1;34m(self, data, rows, cols)\u001B[0m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconvert_to_albumentations\u001B[39m(\u001B[38;5;28mself\u001B[39m, data, rows, cols):\n\u001B[1;32m---> 51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mconvert_bboxes_to_albumentations\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcols\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_validity\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\traffic-sign-recognition\\venv\\lib\\site-packages\\albumentations\\augmentations\\bbox_utils.py:306\u001B[0m, in \u001B[0;36mconvert_bboxes_to_albumentations\u001B[1;34m(bboxes, source_format, rows, cols, check_validity)\u001B[0m\n\u001B[0;32m    304\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconvert_bboxes_to_albumentations\u001B[39m(bboxes, source_format, rows, cols, check_validity\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m    305\u001B[0m     \u001B[38;5;124;03m\"\"\"Convert a list bounding boxes from a format specified in `source_format` to the format used by albumentations\"\"\"\u001B[39;00m\n\u001B[1;32m--> 306\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [convert_bbox_to_albumentations(bbox, source_format, rows, cols, check_validity) \u001B[38;5;28;01mfor\u001B[39;00m bbox \u001B[38;5;129;01min\u001B[39;00m bboxes]\n",
      "File \u001B[1;32m~\\PycharmProjects\\traffic-sign-recognition\\venv\\lib\\site-packages\\albumentations\\augmentations\\bbox_utils.py:306\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    304\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconvert_bboxes_to_albumentations\u001B[39m(bboxes, source_format, rows, cols, check_validity\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m    305\u001B[0m     \u001B[38;5;124;03m\"\"\"Convert a list bounding boxes from a format specified in `source_format` to the format used by albumentations\"\"\"\u001B[39;00m\n\u001B[1;32m--> 306\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43mconvert_bbox_to_albumentations\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbbox\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msource_format\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcols\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_validity\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m bbox \u001B[38;5;129;01min\u001B[39;00m bboxes]\n",
      "File \u001B[1;32m~\\PycharmProjects\\traffic-sign-recognition\\venv\\lib\\site-packages\\albumentations\\augmentations\\bbox_utils.py:254\u001B[0m, in \u001B[0;36mconvert_bbox_to_albumentations\u001B[1;34m(bbox, source_format, rows, cols, check_validity)\u001B[0m\n\u001B[0;32m    252\u001B[0m     bbox \u001B[38;5;241m=\u001B[39m normalize_bbox(bbox, rows, cols)\n\u001B[0;32m    253\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m check_validity:\n\u001B[1;32m--> 254\u001B[0m     \u001B[43mcheck_bbox\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbbox\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m bbox\n",
      "File \u001B[1;32m~\\PycharmProjects\\traffic-sign-recognition\\venv\\lib\\site-packages\\albumentations\\augmentations\\bbox_utils.py:331\u001B[0m, in \u001B[0;36mcheck_bbox\u001B[1;34m(bbox)\u001B[0m\n\u001B[0;32m    329\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, value \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx_min\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my_min\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx_max\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my_max\u001B[39m\u001B[38;5;124m\"\u001B[39m], bbox[:\u001B[38;5;241m4\u001B[39m]):\n\u001B[0;32m    330\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;241m0\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m value \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m np\u001B[38;5;241m.\u001B[39misclose(value, \u001B[38;5;241m0\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m np\u001B[38;5;241m.\u001B[39misclose(value, \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m--> 331\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    332\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected \u001B[39m\u001B[38;5;132;01m{name}\u001B[39;00m\u001B[38;5;124m for bbox \u001B[39m\u001B[38;5;132;01m{bbox}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    333\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto be in the range [0.0, 1.0], got \u001B[39m\u001B[38;5;132;01m{value}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(bbox\u001B[38;5;241m=\u001B[39mbbox, name\u001B[38;5;241m=\u001B[39mname, value\u001B[38;5;241m=\u001B[39mvalue)\n\u001B[0;32m    334\u001B[0m         )\n\u001B[0;32m    335\u001B[0m x_min, y_min, x_max, y_max \u001B[38;5;241m=\u001B[39m bbox[:\u001B[38;5;241m4\u001B[39m]\n\u001B[0;32m    336\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m x_max \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m x_min:\n",
      "\u001B[1;31mValueError\u001B[0m: Expected x_max for bbox (tensor(0.8984), tensor(0.2458), tensor(1.0008), tensor(0.4271), tensor(8)) to be in the range [0.0, 1.0], got 1.0007812976837158."
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = create_model(num_classes=NUM_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "train_loss_hist = Averager()\n",
    "val_loss_hist = Averager()\n",
    "\n",
    "train_itr = 1\n",
    "val_itr = 1\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "if VISUALIZE_TRANSFORMED_IMAGES:\n",
    "    show_transformed_image(train_loader)\n",
    "\n",
    "# start the training epochs\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEPOCH {epoch+1} of {NUM_EPOCHS}\")\n",
    "    train_loss_hist.reset()\n",
    "    val_loss_hist.reset()\n",
    "    # create plot for training progress\n",
    "\n",
    "    figure_1, train_ax = plt.subplots()\n",
    "    figure_2, valid_ax = plt.subplots()\n",
    "\n",
    "    start = time.time()\n",
    "    train_loss = train(train_loader, model)\n",
    "    val_loss = validate(valid_loader, model)\n",
    "    print(f\"Epoch #{epoch} train loss: {train_loss_hist.value:.3f}\")\n",
    "    print(f\"Epoch #{epoch} validation loss: {val_loss_hist.value:.3f}\")\n",
    "    end = time.time()\n",
    "    print(f\"Took {((end - start) / 60):.3f} minutes for epoch {epoch}\")\n",
    "    if (epoch+1) % SAVE_MODEL_EPOCH == 0:\n",
    "        torch.save(model.state_dict(), f\"{OUT_DIR}/model{epoch+1}.pth\")\n",
    "        print('SAVING MODEL COMPLETE...\\n')\n",
    "\n",
    "    if (epoch+1) % SAVE_PLOTS_EPOCH == 0:\n",
    "        train_ax.plot(train_loss, color='blue')\n",
    "        train_ax.set_xlabel('iterations')\n",
    "        train_ax.set_ylabel('train loss')\n",
    "        valid_ax.plot(val_loss, color='red')\n",
    "        valid_ax.set_xlabel('iterations')\n",
    "        valid_ax.set_ylabel('validation loss')\n",
    "        figure_1.savefig(f\"{OUT_DIR}/train_loss_{epoch+1}.png\")\n",
    "        figure_2.savefig(f\"{OUT_DIR}/valid_loss_{epoch+1}.png\")\n",
    "        print('SAVING PLOTS COMPLETE...')\n",
    "\n",
    "    if (epoch+1) == NUM_EPOCHS:\n",
    "        train_ax.plot(train_loss, color='blue')\n",
    "        train_ax.set_xlabel('iterations')\n",
    "        train_ax.set_ylabel('train loss')\n",
    "        valid_ax.plot(val_loss, color='red')\n",
    "        valid_ax.set_xlabel('iterations')\n",
    "        valid_ax.set_ylabel('validation loss')\n",
    "        figure_1.savefig(f\"{OUT_DIR}/train_loss_{epoch+1}.png\")\n",
    "        figure_2.savefig(f\"{OUT_DIR}/valid_loss_{epoch+1}.png\")\n",
    "        torch.save(model.state_dict(), f\"{OUT_DIR}/model{epoch+1}.pth\")\n",
    "\n",
    "    plt.close('all')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}